"""Initial baseline from existing database

Revision ID: e796582016c8
Revises: 
Create Date: 2025-09-22 09:54:48.217426

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'e796582016c8'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('analysis_presets')
    op.drop_table('basetablemodel')
    op.drop_index(op.f('idx_run_items_item_id'), table_name='analysis_run_items')
    op.drop_index(op.f('idx_run_items_run_id'), table_name='analysis_run_items')
    op.drop_index(op.f('idx_run_items_state'), table_name='analysis_run_items')
    op.drop_table('analysis_run_items')
    op.drop_index(op.f('idx_item_analysis_impact_overall'), table_name='item_analysis')
    op.drop_index(op.f('idx_item_analysis_sentiment_label'), table_name='item_analysis')
    op.drop_index(op.f('idx_item_analysis_updated'), table_name='item_analysis')
    op.drop_index(op.f('idx_item_analysis_urgency'), table_name='item_analysis')
    op.drop_table('item_analysis')
    op.drop_index(op.f('idx_analysis_runs_created'), table_name='analysis_runs')
    op.drop_index(op.f('idx_analysis_runs_scope_hash'), table_name='analysis_runs')
    op.drop_index(op.f('idx_analysis_runs_status'), table_name='analysis_runs')
    op.drop_table('analysis_runs')
    op.add_column('categories', sa.Column('updated_at', sa.DateTime(), nullable=True))
    op.alter_column('content_processing_logs', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('dynamic_feed_templates', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True)
    op.drop_column('dynamic_feed_templates', 'usage_count')
    op.drop_column('dynamic_feed_templates', 'last_used')
    op.alter_column('feed_categories', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('feed_configuration_changes', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('feed_health', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('feed_health', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True)
    op.alter_column('feed_processor_configs', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True)
    op.alter_column('feed_scheduler_state', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True)
    op.alter_column('feed_template_assignments', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True)
    op.add_column('feed_types', sa.Column('updated_at', sa.DateTime(), nullable=True))
    op.alter_column('item_tags', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('processor_templates', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True)
    op.add_column('sources', sa.Column('updated_at', sa.DateTime(), nullable=True))
    op.alter_column('user_settings', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('user_settings', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False)
    op.drop_column('sources', 'updated_at')
    op.alter_column('processor_templates', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False)
    op.alter_column('item_tags', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_column('feed_types', 'updated_at')
    op.alter_column('feed_template_assignments', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False)
    op.alter_column('feed_scheduler_state', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False)
    op.alter_column('feed_processor_configs', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False)
    op.alter_column('feed_health', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False)
    op.alter_column('feed_health', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('now()'))
    op.alter_column('feed_configuration_changes', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('feed_categories', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.add_column('dynamic_feed_templates', sa.Column('last_used', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    op.add_column('dynamic_feed_templates', sa.Column('usage_count', sa.INTEGER(), server_default=sa.text('0'), autoincrement=False, nullable=True))
    op.alter_column('dynamic_feed_templates', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False)
    op.alter_column('content_processing_logs', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('now()'))
    op.drop_column('categories', 'updated_at')
    op.create_table('analysis_runs',
    sa.Column('id', sa.BIGINT(), server_default=sa.text("nextval('analysis_runs_id_seq'::regclass)"), autoincrement=True, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.Column('scope_json', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=False),
    sa.Column('params_json', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=False),
    sa.Column('scope_hash', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('status', sa.TEXT(), server_default=sa.text("'pending'::text"), autoincrement=False, nullable=False),
    sa.Column('queued_count', sa.INTEGER(), server_default=sa.text('0'), autoincrement=False, nullable=False),
    sa.Column('processed_count', sa.INTEGER(), server_default=sa.text('0'), autoincrement=False, nullable=False),
    sa.Column('failed_count', sa.INTEGER(), server_default=sa.text('0'), autoincrement=False, nullable=False),
    sa.Column('cost_estimate', sa.NUMERIC(precision=10, scale=4), server_default=sa.text('0.0'), autoincrement=False, nullable=True),
    sa.Column('actual_cost', sa.NUMERIC(precision=10, scale=4), server_default=sa.text('0.0'), autoincrement=False, nullable=True),
    sa.Column('error_rate', sa.NUMERIC(precision=5, scale=4), server_default=sa.text('0.0'), autoincrement=False, nullable=True),
    sa.Column('items_per_min', sa.NUMERIC(precision=8, scale=2), server_default=sa.text('0.0'), autoincrement=False, nullable=True),
    sa.Column('eta_seconds', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('coverage_10m', sa.NUMERIC(precision=5, scale=4), server_default=sa.text('0.0'), autoincrement=False, nullable=True),
    sa.Column('coverage_60m', sa.NUMERIC(precision=5, scale=4), server_default=sa.text('0.0'), autoincrement=False, nullable=True),
    sa.Column('started_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('completed_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('last_error', sa.TEXT(), autoincrement=False, nullable=True),
    sa.CheckConstraint("status = ANY (ARRAY['pending'::text, 'running'::text, 'paused'::text, 'completed'::text, 'failed'::text, 'cancelled'::text])", name='analysis_runs_status_check'),
    sa.PrimaryKeyConstraint('id', name='analysis_runs_pkey'),
    postgresql_ignore_search_path=False
    )
    op.create_index(op.f('idx_analysis_runs_status'), 'analysis_runs', ['status'], unique=False)
    op.create_index(op.f('idx_analysis_runs_scope_hash'), 'analysis_runs', ['scope_hash'], unique=False)
    op.create_index(op.f('idx_analysis_runs_created'), 'analysis_runs', [sa.literal_column('created_at DESC')], unique=False)
    op.create_table('item_analysis',
    sa.Column('item_id', sa.BIGINT(), autoincrement=False, nullable=False),
    sa.Column('sentiment_json', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=False, comment='JSON containing overall sentiment, urgency, and themes'),
    sa.Column('impact_json', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=False, comment='JSON containing impact score and volatility metrics'),
    sa.Column('model_tag', sa.TEXT(), autoincrement=False, nullable=True, comment='LLM model identifier used for analysis'),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.ForeignKeyConstraint(['item_id'], ['items.id'], name=op.f('item_analysis_item_id_fkey'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('item_id', name=op.f('item_analysis_pkey')),
    comment='Stores LLM-generated sentiment and impact analysis for news items'
    )
    op.create_index(op.f('idx_item_analysis_urgency'), 'item_analysis', [sa.literal_column("((sentiment_json ->> 'urgency'::text)::numeric)")], unique=False)
    op.create_index(op.f('idx_item_analysis_updated'), 'item_analysis', [sa.literal_column('updated_at DESC')], unique=False)
    op.create_index(op.f('idx_item_analysis_sentiment_label'), 'item_analysis', [sa.literal_column("((sentiment_json -> 'overall'::text) ->> 'label'::text)")], unique=False)
    op.create_index(op.f('idx_item_analysis_impact_overall'), 'item_analysis', [sa.literal_column("((impact_json ->> 'overall'::text)::numeric)")], unique=False)
    op.create_table('analysis_run_items',
    sa.Column('id', sa.BIGINT(), autoincrement=True, nullable=False),
    sa.Column('run_id', sa.BIGINT(), autoincrement=False, nullable=False),
    sa.Column('item_id', sa.BIGINT(), autoincrement=False, nullable=False),
    sa.Column('state', sa.TEXT(), server_default=sa.text("'queued'::text"), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.Column('started_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('completed_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('error_message', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('tokens_used', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('cost_usd', sa.NUMERIC(precision=8, scale=6), server_default=sa.text('NULL::numeric'), autoincrement=False, nullable=True),
    sa.CheckConstraint("state = ANY (ARRAY['queued'::text, 'processing'::text, 'completed'::text, 'failed'::text, 'skipped'::text])", name=op.f('analysis_run_items_state_check')),
    sa.ForeignKeyConstraint(['item_id'], ['items.id'], name=op.f('analysis_run_items_item_id_fkey'), ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['run_id'], ['analysis_runs.id'], name=op.f('analysis_run_items_run_id_fkey'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('analysis_run_items_pkey')),
    sa.UniqueConstraint('run_id', 'item_id', name=op.f('analysis_run_items_run_id_item_id_key'), postgresql_include=[], postgresql_nulls_not_distinct=False)
    )
    op.create_index(op.f('idx_run_items_state'), 'analysis_run_items', ['state'], unique=False)
    op.create_index(op.f('idx_run_items_run_id'), 'analysis_run_items', ['run_id'], unique=False)
    op.create_index(op.f('idx_run_items_item_id'), 'analysis_run_items', ['item_id'], unique=False)
    op.create_table('basetablemodel',
    sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('basetablemodel_pkey'))
    )
    op.create_table('analysis_presets',
    sa.Column('id', sa.BIGINT(), autoincrement=True, nullable=False),
    sa.Column('name', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('description', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('scope_json', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=False),
    sa.Column('params_json', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('analysis_presets_pkey')),
    sa.UniqueConstraint('name', name=op.f('analysis_presets_name_key'), postgresql_include=[], postgresql_nulls_not_distinct=False)
    )
    # ### end Alembic commands ###
